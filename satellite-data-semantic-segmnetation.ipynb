{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1196732,"sourceType":"datasetVersion","datasetId":681625}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install patchify opencv-python tensorflow numpy scikit-learn --upgrade","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport cv2\nimport os\nimport shutil\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom patchify import patchify\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport random","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"minmaxscaler = MinMaxScaler()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for path, subdirs, files in os.walk('/kaggle/input'):\n  dir_name = path.split(os.path.sep)[-1]\n  print(dir_name)\n  if dir_name == 'images':\n    images = os.listdir(path)\n    print(path)\n    for i, image_name in enumerate(images):\n      if(image_name.endswith('jpg')):\n        print(image_name)\n        a = True","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image = cv2.imread('/kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 1/images/image_part_001.jpg',1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"type(image)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"type(Image.fromarray(image))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image\nimage.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_patch_size = 256","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_patches = patchify(image,(image_patch_size, image_patch_size,3),step = image_patch_size)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_x = image_patches[0,0,:,:]\nimages_y = minmaxscaler.fit_transform(image_x.reshape(-1, image_x.shape[-1])).reshape(image_x.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(image_patches.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_dataset = []\nmask_dataset = []\nfor image_type in ['images' , 'masks']:\n  if image_type == 'images':\n    image_ex = 'jpg'\n  elif image_type == 'masks':\n     image_ex = 'png'\n  for tile_id in range(1,9):\n    for image_id in range(1,10):\n      image = cv2.imread(f'/kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile {tile_id}/{image_type}/image_part_00{image_id}.{image_ex}',1)\n      if image is not None:\n        if image_type == 'masks':\n          image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        #print(image.shape)\n        size_x = (image.shape[1]//image_patch_size)*image_patch_size\n        size_y = (image.shape[0]//image_patch_size)*image_patch_size\n        #print(\"{} --- {} - {}\".format(image.shape, size_x, size_y))\n        image = Image.fromarray(image)\n        image = image.crop((0,0, size_x, size_y))\n        #print(\"({},  {})\".format(image.size[0],image.size[1]))\n        image = np.array(image)\n        patched_images = patchify(image, (image_patch_size, image_patch_size, 3), step=image_patch_size)\n        #print(len(patched_images))\n        for i in range(patched_images.shape[0]):\n          for j in range(patched_images.shape[1]):\n            if image_type == 'images':\n              individual_patched_image = patched_images[i,j,:,:]\n              #print(individual_patched_image.shape)\n              individual_patched_image = minmaxscaler.fit_transform(individual_patched_image.reshape(-1, individual_patched_image.shape[-1])).reshape(individual_patched_image.shape)\n              individual_patched_image = individual_patched_image[0]\n              #print(individual_patched_image.shape)\n              image_dataset.append(individual_patched_image)\n            elif image_type == 'masks':\n              individual_patched_mask = patched_images[i,j,:,:]\n              individual_patched_mask = individual_patched_mask[0]\n              mask_dataset.append(individual_patched_mask)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(len(image_dataset))\nprint(len(mask_dataset))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mask_dataset[0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_dataset = np.array(image_dataset)\nmask_dataset = np.array(mask_dataset)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"random_image_num = random.randint(0,len(image_dataset))\n\nplt.figure(figsize = (15,10))\nplt.subplot(121)\nplt.imshow(image_dataset[random_image_num])\nplt.subplot(122)\nplt.imshow(mask_dataset[random_image_num])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class_building = '#3C1098'\nclass_building = class_building.lstrip(\"#\")\nclass_building = np.array(tuple(int(class_building[i:i+2], 16) for i in (0,2,4)))\nprint(class_building)\n\nclass_land = '#8429F6'\nclass_land = class_land.lstrip(\"#\")\nclass_land = np.array(tuple(int(class_land[i:i+2], 16) for i in (0,2,4)))\nprint(class_land)\n\nclass_vegetation = '#FEDD3A'\nclass_vegetation = class_vegetation.lstrip(\"#\")\nclass_vegetation = np.array(tuple(int(class_vegetation[i:i+2], 16) for i in (0,2,4)))\nprint(class_vegetation)\n\nclass_road = '#6EC1E4'\nclass_road = class_road.lstrip(\"#\")\nclass_road = np.array(tuple(int(class_road[i:i+2], 16) for i in (0,2,4)))\nprint(class_road)\n\nclass_water = '#E2A929'\nclass_water = class_water.lstrip(\"#\")\nclass_water = np.array(tuple(int(class_water[i:i+2], 16) for i in (0,2,4)))\nprint(class_water)\n\nclass_unlabeled = '#9B9B9B'\nclass_unlabeled = class_unlabeled.lstrip(\"#\")\nclass_unlabeled = np.array(tuple(int(class_unlabeled[i:i+2], 16) for i in (0,2,4)))\nprint(class_unlabeled)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mask_dataset.shape[0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"label = individual_patched_mask","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def rgb_to_label(label):\n  label_segment = np.zeros(label.shape, dtype=np.uint8)\n  label_segment[np.all(label == class_water, axis=-1)] = 0\n  label_segment[np.all(label == class_land, axis=-1)] = 1\n  label_segment[np.all(label == class_road, axis=-1)] = 2\n  label_segment[np.all(label == class_building, axis=-1)] = 3\n  label_segment[np.all(label == class_vegetation, axis=-1)] = 4\n  label_segment[np.all(label == class_unlabeled, axis=-1)] = 5\n  #print(label_segment)\n  label_segment = label_segment[:,:,0]\n  #print(label_segment)\n  return label_segment","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"labels = []\nfor i in range(mask_dataset.shape[0]):\n  label = rgb_to_label(mask_dataset[i])\n  labels.append(label)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(len(labels))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"labels = np.array(labels)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"labels[0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"labels = np.expand_dims(labels,axis=3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"labels[1][2]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.unique(labels)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"random_image_num = random.randint(0,len(image_dataset))\n\nplt.figure(figsize = (15,10))\nplt.subplot(121)\nplt.imshow(image_dataset[random_image_num])\nplt.subplot(122)\nplt.imshow(labels[random_image_num][:,:,0])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"training_set = image_dataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.utils import to_categorical\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"labels_categorical_data = to_categorical(labels,num_classes = len(np.unique(labels)))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"labels_categorical_data.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"training_set.shape\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(training_set,labels_categorical_data,test_size = 0.2,random_state = 100)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_height = x_train.shape[1]\nimage_width = x_train.shape[2]\nimage_channels = x_train.shape[3]\nimage_classes = y_train.shape[3]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Conv2DTranspose\nfrom tensorflow.keras.layers import concatenate, BatchNormalization, Dropout, Lambda","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras import backend as K","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def jaccard_coef(y_true, y_pred):\n  y_true_flatten = K.flatten(y_true)\n  y_pred_flatten = K.flatten(y_pred)\n  intersection = K.sum(y_true_flatten * y_pred_flatten)\n  final_coef_value = (intersection + 1.0) / (K.sum(y_true_flatten) + K.sum(y_pred_flatten) - intersection + 1.0)\n  return final_coef_value","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def multi_unet_model(n_classes=5,\n                     image_height=256,\n                     image_width=256,\n                     image_channels=1):\n\n    # -------- Encoder --------\n    inputs = Input(shape=(image_height, image_width, image_channels))\n\n    c1 = Conv2D(16,  (3, 3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(inputs)\n    c1 = Dropout(0.2)(c1)\n    c1 = Conv2D(16,  (3, 3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(c1)\n    p1 = MaxPooling2D((2, 2))(c1)\n\n    c2 = Conv2D(32,  (3, 3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(p1)\n    c2 = Dropout(0.2)(c2)\n    c2 = Conv2D(32,  (3, 3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(c2)\n    p2 = MaxPooling2D((2, 2))(c2)\n\n    c3 = Conv2D(64,  (3, 3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(p2)\n    c3 = Dropout(0.2)(c3)\n    c3 = Conv2D(64,  (3, 3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(c3)\n    p3 = MaxPooling2D((2, 2))(c3)\n\n    c4 = Conv2D(128, (3, 3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(p3)\n    c4 = Dropout(0.2)(c4)\n    c4 = Conv2D(128, (3, 3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(c4)\n    p4 = MaxPooling2D((2, 2))(c4)\n\n    c5 = Conv2D(256, (3, 3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(p4)\n    c5 = Dropout(0.2)(c5)\n    c5 = Conv2D(256, (3, 3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(c5)\n\n    # -------- Decoder --------\n    u6 = Conv2DTranspose(128, (3, 3), strides=(2, 2), padding=\"same\")(c5)\n    u6 = concatenate([u6, c4])\n    c6 = Conv2D(128, (3, 3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(u6)\n    c6 = Dropout(0.2)(c6)\n    c6 = Conv2D(128, (3, 3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(c6)\n\n    u7 = Conv2DTranspose(64, (3, 3), strides=(2, 2), padding=\"same\")(c6)\n    u7 = concatenate([u7, c3])\n    c7 = Conv2D(64, (3, 3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(u7)\n    c7 = Dropout(0.2)(c7)\n    c7 = Conv2D(64, (3, 3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(c7)\n\n    u8 = Conv2DTranspose(32, (3, 3), strides=(2, 2), padding=\"same\")(c7)\n    u8 = concatenate([u8, c2])\n    c8 = Conv2D(32, (3, 3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(u8)\n    c8 = Dropout(0.2)(c8)\n    c8 = Conv2D(32, (3, 3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(c8)\n\n    u9 = Conv2DTranspose(16, (3, 3), strides=(2, 2), padding=\"same\")(c8)\n    u9 = concatenate([u9, c1], axis=3)\n    c9 = Conv2D(16, (3, 3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(u9)\n    c9 = Dropout(0.2)(c9)\n    c9 = Conv2D(16, (3, 3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(c9)\n\n    outputs = Conv2D(n_classes, (1, 1), activation=\"softmax\")(c9)\n\n    model = Model(inputs=inputs, outputs=outputs)\n    return model\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"metrics = ['accuracy', jaccard_coef]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_model():\n  return multi_unet_model(n_classes=image_classes ,\n                          image_height=image_height,\n                          image_width=image_width,\n                          image_channels=image_channels)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = get_model()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.get_config()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"weights = [0.166,0.166,0.166,0.166,0.166,0.166]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip uninstall tensorflow -y","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install tensorflow==2.15.0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install segmentation-models","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.environ['SM_FRAMEWORK'] = 'tf.keras'  # Set environment variable for segmentation_models\nimport segmentation_models as sm\n\n# Verify framework\nprint(sm.__version__)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dice_loss = sm.losses.DiceLoss(class_weights = weights)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"focal_loss  = sm.losses.CategoricalFocalLoss()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"total_loss = dice_loss + (1*focal_loss)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tf.keras.backend.clear_session()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.compile(optimizer ='adam',\n              loss = total_loss,\n             metrics = metrics)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_history = model.fit(x_train,y_train,\n                         batch_size=16,\n                         verbose = 1,\n                         epochs = 10,\n                         validation_data=(x_test,y_test),\n                         shuffle = False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"loss = model_history.history['loss']\nval_loss = model_history.history['val_loss']\nepochs = range(1,len(loss)+1 )\nplt.plot(epochs,loss,'y', label = 'training_loss')\nplt.plot(epochs,val_loss,'r', label = 'validation_loss') \nplt.title('Training loss vs. Validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"jaccard_coef = model_history.history['jaccard_coef']\nval_jaccard_coef = model_history.history['val_jaccard_coef']\nepochs = range(1,len(jaccard_coef )+1 )\nplt.plot(epochs,jaccard_coef,'y', label = 'Training IoU')\nplt.plot(epochs,val_jaccard_coef,'r', label = 'Validation IoU') \nplt.title('Training IoU vs. Validation IoU')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred = model.predict(x_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred_argmax = np.argmax(y_pred,axis=3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred_argmax","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(y_pred_argmax)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_test_argmax = np.argmax(y_test,axis=3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_test_argmax","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_img_num = 10\ntest_img = x_test[test_img_num]\nground_truth_image = y_test_argmax[test_img_num]\n \ntest_image_input = np.expand_dims(test_img,0)\n\nprediction = model.predict(test_image_input)\npredicted_image = np.argmax(prediction,axis=3)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predicted_image","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predicted_image = predicted_image[0,:,:]\npredicted_image","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(fig_size =(14,8))\nplt.subplot(231)\nplt.title('Original Test Image')\nplt.imshow(test_img)\n\nplt.subplot(232)\nplt.title('Ground Truth Image')\nplt.imshow(ground_truth_image)\n\nplt.subplot(232)\nplt.title('Predicted Image')\nplt.imshow(predicted_image)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.imshow(test_img)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.imshow(ground_truth_image)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.imshow(predicted_image)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save('satellite_seg_model.h5')","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}